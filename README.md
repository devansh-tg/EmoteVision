<p align="center">
  <img src="https://img.shields.io/badge/Python-3.11-3776AB?logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/TensorFlow-2.x-FF6F00?logo=tensorflow&logoColor=white" />
  <img src="https://img.shields.io/badge/Flask-3.x-000000?logo=flask&logoColor=white" />
  <img src="https://img.shields.io/badge/OpenCV-4.x-5C3EE8?logo=opencv&logoColor=white" />
  <img src="https://img.shields.io/badge/Socket.IO-4.x-010101?logo=socketdotio&logoColor=white" />
  <img src="https://img.shields.io/badge/License-MIT-green" />
</p>

<h1 align="center">ğŸ­ EmoteVision</h1>
<p align="center">
  <strong>Real-time facial emotion detection powered by deep learning</strong><br>
  Live webcam analysis Â· WebSocket streaming Â· 3D glassmorphism UI
</p>

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| **Real-Time Detection** | Detects 7 emotions (Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise) from webcam feed at ~15-30 FPS |
| **Deep Learning CNN** | Custom 4-block convolutional neural network (64â†’128â†’256â†’512 filters) trained on 35,000+ images |
| **WebSocket Streaming** | Flask-SocketIO pushes emotion data instantly â€” no polling, sub-100ms latency |
| **3D Interactive UI** | Glassmorphic cards with mouse-tilt perspective, particle neural network background, neon glow system |
| **Live Analytics** | Confidence bars, emotion trend chart (Chart.js), engagement gauge, inference timer |
| **Session Management** | Export session data as CSV, reset stats, persistent theme preference |
| **CLAHE Preprocessing** | Adaptive histogram equalization at inference for robust detection in varied lighting |
| **EWA Smoothing** | Exponential weighted average reduces emotion flicker for stable real-time output |
| **Dark / Light Theme** | Full theme toggle with localStorage persistence |
| **Keyboard Shortcuts** | `R` = manual predict, `H` = home |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    WebSocket     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Flask-SocketIO   â”‚
â”‚  (JS + CSS)  â”‚   emotion_update â”‚    (app.py)       â”‚
â”‚              â”‚â—„â”€â”€â”€ MJPEG â”€â”€â”€â”€â”€â”€â”‚                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚  EmotionDetector    â”‚
                                  â”‚  (utils/detector.py)â”‚
                                  â”‚                     â”‚
                                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                  â”‚  â”‚ Keras / TFLiteâ”‚  â”‚
                                  â”‚  â”‚   CNN Model   â”‚  â”‚
                                  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                  â”‚  â”‚  Haar / Media- â”‚  â”‚
                                  â”‚  â”‚  Pipe Face Detâ”‚  â”‚
                                  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                  â”‚  â”‚ CLAHE + EWA   â”‚  â”‚
                                  â”‚  â”‚ Preprocessing â”‚  â”‚
                                  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
src/
â”œâ”€â”€ app.py                 # Flask-SocketIO server (routes, WebSocket, MJPEG stream)
â”œâ”€â”€ config.py              # Centralized configuration (env-var overridable)
â”œâ”€â”€ train.py               # Model training script (argparse, callbacks, confusion matrix)
â”œâ”€â”€ test.py                # Quick webcam test with emoji overlay
â”œâ”€â”€ export_tflite.py       # Convert .h5 â†’ .tflite with optional quantization
â”œâ”€â”€ model.h5               # Trained Keras model (~68-70% val accuracy on FER-2013)
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env.example           # Environment variable reference
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ detector.py        # EmotionDetector class (inference, smoothing, drawing)
â”‚   â””â”€â”€ frame_buffer.py    # Thread-safe singleton camera buffer
â”‚
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ style.css          # 3D glassmorphism UI (dark/light themes)
â”‚   â””â”€â”€ script.js          # Socket.IO client, Chart.js, particles, 3D tilt
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html         # Main detection page
â”‚   â””â”€â”€ about.html         # About / tech stack page
â”‚
â”œâ”€â”€ emojis/                # 7 emotion emoji PNGs
â”‚   â”œâ”€â”€ angry.png ... surprise.png
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_detector.py   # Pytest unit tests for detector & config
â”‚
â””â”€â”€ data/                  # FER-2013 dataset (not included in repo)
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ angry/ ... surprise/
    â””â”€â”€ test/
        â”œâ”€â”€ angry/ ... surprise/
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+** (tested on 3.11)
- **Webcam** (built-in or USB)
- ~2 GB disk space (TensorFlow)

### 1. Clone the repository

```bash
git clone https://github.com/devansh-tg/emojify--copy-.git
cd emojify--copy-/src
```

### 2. Create a virtual environment (recommended)

```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# macOS / Linux
source .venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Run the web app

```bash
python app.py
```

Open **http://127.0.0.1:5000** in your browser. The model takes ~60-80s to load on first run (TensorFlow initialization).

---

## ğŸ§  Model Details

| Property | Value |
|----------|-------|
| **Architecture** | Sequential CNN â€” 4 conv blocks + GlobalAveragePooling + 2 Dense layers |
| **Input** | 48Ã—48 grayscale face crop |
| **Output** | 7-class softmax (Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise) |
| **Parameters** | ~2.5M |
| **Training Data** | FER-2013 (35,887 images) |
| **Validation Accuracy** | ~68-70% (human agreement on FER-2013 is ~65-72%) |
| **Optimizer** | Adam (lr=2e-4) |
| **Regularization** | L2 (5e-4), Dropout (0.20â€“0.40), BatchNorm |
| **Augmentation** | Rotation, shift, flip, zoom, brightness, channel shift |

### Conv Block Structure
```
Conv2D â†’ BatchNorm â†’ Conv2D â†’ BatchNorm â†’ MaxPool â†’ Dropout
Filters: 64 â†’ 128 â†’ 256 â†’ 512
```

### Training

To retrain from scratch (requires dataset in `data/` folder):

```bash
python train.py --train-dir data/train --val-dir data/test --epochs 150 --batch-size 64
```

**Output files:**
- `model.h5` / `model_best.h5` â€” trained model
- `model_meta.json` â€” accuracy metrics + metadata
- `confusion_matrix.png` â€” per-class detection heatmap
- `classification_report.txt` â€” precision / recall / F1
- `training_history.png` â€” accuracy, loss, gap curves

### Export to TFLite (optional, for faster inference)

```bash
python export_tflite.py --input model.h5 --output model.tflite --quantize dynamic
```

---

## âš™ï¸ Configuration

All settings are configurable via **environment variables** or the `.env` file. See [.env.example](src/.env.example) for the full list.

| Variable | Default | Description |
|----------|---------|-------------|
| `EMOTEVISION_MODEL` | `model.h5` | Path to Keras model |
| `EMOTEVISION_TFLITE` | `model.tflite` | Path to TFLite model |
| `EMOTEVISION_USE_TFLITE` | `auto` | `true`, `false`, or `auto` |
| `EMOTEVISION_CAMERA` | `0` | Camera index |
| `EMOTEVISION_HOST` | `0.0.0.0` | Server bind address |
| `EMOTEVISION_PORT` | `5000` | Server port |
| `EMOTEVISION_SMOOTHING` | `0.4` | EWA smoothing alpha (0=max smooth, 1=no smooth) |

---

## ğŸ§ª Testing

```bash
pytest tests/ -v --tb=short
```

Tests cover:
- Detector initialization and model loading
- Emotion prediction output format
- Engagement score calculation
- Configuration validation

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| **Backend** | Python 3.11, Flask 3.x, Flask-SocketIO 5.x |
| **Deep Learning** | TensorFlow / Keras 2.x, custom CNN |
| **Computer Vision** | OpenCV 4.x, Haar Cascade (default) or MediaPipe |
| **Frontend** | Vanilla JS, CSS3 (custom properties, glassmorphism, keyframe animations) |
| **Charts** | Chart.js 4.x (60-point sliding trend) |
| **Real-Time** | Socket.IO 4.x (WebSocket transport) |
| **Preprocessing** | CLAHE (adaptive contrast), EWA temporal smoothing |

---

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| **Inference time** | ~15-40ms per frame (CPU) |
| **FPS** | ~15-30 FPS |
| **Model load** | ~60-80s (first run, TensorFlow CPU init) |
| **Memory** | ~400-600 MB (TensorFlow + OpenCV) |
| **WebSocket latency** | <50ms |

---

## ğŸ“„ License

This project is for educational purposes. The FER-2013 dataset is provided under its own license via Kaggle.

---

<p align="center">
  Built with â¤ï¸ by <a href="https://github.com/devansh-tg">Devansh</a>
</p>
